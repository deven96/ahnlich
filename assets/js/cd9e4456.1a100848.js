"use strict";(self.webpackChunkahnlich_web=self.webpackChunkahnlich_web||[]).push([[3773],{28453:(e,i,n)=>{n.d(i,{R:()=>l,x:()=>o});var r=n(96540);const t={},s=r.createContext(t);function l(e){const i=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),r.createElement(s.Provider,{value:i},e.children)}},72787:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>d,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>r,toc:()=>a});const r=JSON.parse('{"id":"client-libraries/go/request-ai/request-ai","title":"Request AI","description":"Ahnlich Request-ai: AI proxy to communicate with ahnlich-db, receiving raw input, transforming it into embeddings, and storing those embeddings within the DB. It extends DB capabilities by allowing developers to issue queries to the same store using raw inputs such as images or text. The AI proxy features multiple off-the-shelf models which can be selected for both store indexing and query time.","source":"@site/docs/client-libraries/go/request-ai/request-ai.md","sourceDirName":"client-libraries/go/request-ai","slug":"/client-libraries/go/request-ai/","permalink":"/docs/client-libraries/go/request-ai/","draft":false,"unlisted":false,"editUrl":"https://github.com/deven96/ahnlich/tree/main/web/ahnlich-web/docs/client-libraries/go/request-ai/request-ai.md","tags":[],"version":"current","frontMatter":{"title":"Request AI","sidebar_posiiton":3},"sidebar":"docsSidebar","previous":{"title":"Delete Predicate","permalink":"/docs/client-libraries/go/request-db/delete-predicate"},"next":{"title":"Ping","permalink":"/docs/client-libraries/go/request-ai/ping"}}');var t=n(74848),s=n(28453);const l={title:"Request AI",sidebar_posiiton:3},o="Request - AI",d={},a=[{value:"When to use Ahnlich Request AI",id:"when-to-use-ahnlich-request-ai",level:2},{value:"How it works",id:"how-it-works",level:2},{value:"Model configuration",id:"model-configuration",level:2},{value:"Behavior and expectations",id:"behavior-and-expectations",level:2},{value:"Source Code Example",id:"source-code-example",level:2}];function c(e){const i={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(i.header,{children:(0,t.jsx)(i.h1,{id:"request---ai",children:"Request - AI"})}),"\n",(0,t.jsx)(i.p,{children:"Ahnlich Request-ai: AI proxy to communicate with ahnlich-db, receiving raw input, transforming it into embeddings, and storing those embeddings within the DB. It extends DB capabilities by allowing developers to issue queries to the same store using raw inputs such as images or text. The AI proxy features multiple off-the-shelf models which can be selected for both store indexing and query time."}),"\n",(0,t.jsx)(i.p,{children:"Ahnlich Request-AI acts as a bridge between raw developer inputs (text, images, etc.) and the vector store. Instead of requiring clients to precompute embeddings, the AI proxy accepts raw inputs, runs a model to create embeddings, and persists those embeddings into the target Ahnlich DB store. Once stored, the same store can be queried using raw input \u2014 the proxy will convert the query into an embedding and run the search on the DB."}),"\n",(0,t.jsx)(i.h2,{id:"when-to-use-ahnlich-request-ai",children:"When to use Ahnlich Request AI"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:"Simplify client logic: let the proxy handle embedding generation so clients send raw content (text/images) rather than precomputed vectors."}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:"Faster prototyping: quickly add semantic search by selecting an off-the-shelf model without changing client code."}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:"Model selection flexibility: pick different models for indexing and querying to suit accuracy/latency tradeoffs."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(i.h2,{id:"how-it-works",children:"How it works"}),"\n",(0,t.jsxs)(i.ol,{children:["\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:"Receive raw input \u2014 the AI proxy accepts raw payloads (for example, text or image data)."}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:"Transform to embeddings \u2014 the proxy runs the selected model to generate vector embeddings for the input."}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:"Store in DB \u2014 embeddings plus any associated metadata are stored in the specified Ahnlich DB store."}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:"Query with raw input \u2014 when you issue a query using raw input, the proxy repeats steps 1\u20133 for the query and forwards results."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(i.h2,{id:"model-configuration",children:"Model configuration"}),"\n",(0,t.jsx)(i.p,{children:"When creating or configuring a store via the AI proxy, you supply two model identifiers:"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:"INDEXMODEL \u2014 the model used to generate embeddings for items that will be stored (indexing)."}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:"QUERYMODEL \u2014 the model used to transform incoming raw queries into embeddings for search."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(i.p,{children:(0,t.jsx)(i.em,{children:"Example (CLI-style):"})}),"\n",(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{className:"language-go",children:"CREATESTORE my_store QUERYMODEL all-minilm-l6-v2 INDEXMODEL all-minilm-l6-v2\n"})}),"\n",(0,t.jsx)(i.p,{children:"Structured example (provided):"}),"\n",(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{className:"language-go",children:'create_store(\n    store="my_store",\n    index_model="all-minilm-l6-v2",\n    query_model="all-minilm-l6-v2",\n)\n'})}),"\n",(0,t.jsx)(i.h2,{id:"behavior-and-expectations",children:"Behavior and expectations"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:"The AI proxy generates embeddings on behalf of the client and persists them into the DB, so clients do not need to manage embedding computation."}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsx)(i.p,{children:"The proxy supports multiple off-the-shelf models; you select the model identifiers when creating or configuring a store."}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:["After indexing, the same store can be queried using raw input; the proxy will convert the query into an embedding using the configured ",(0,t.jsx)(i.code,{children:"query_model"})," and forward the similarity request to the DB."]}),"\n"]}),"\n",(0,t.jsxs)(i.li,{children:["\n",(0,t.jsxs)(i.p,{children:["Choosing different ",(0,t.jsx)(i.code,{children:"index_model"})," and ",(0,t.jsx)(i.code,{children:"query_model"})," is supported (the example uses the same model for both), enabling flexibility in balancing index-time embedding quality vs. query-time performance."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(i.h2,{id:"source-code-example",children:"Source Code Example"}),"\n",(0,t.jsx)(i.p,{children:"Create a store named my_store and select the same model for both indexing and querying:"}),"\n",(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{className:"language-go",children:"CREATESTORE my_store QUERYMODEL all-minilm-l6-v2 INDEXMODEL all-minilm-l6-v2\n"})}),"\n",(0,t.jsx)(i.p,{children:"Equivalent in structured form:"}),"\n",(0,t.jsx)(i.pre,{children:(0,t.jsx)(i.code,{className:"language-go",children:'create_store(\n    store="my_store",\n    index_model="all-minilm-l6-v2",\n    query_model="all-minilm-l6-v2",\n)\n'})}),"\n",(0,t.jsx)(i.p,{children:"Below is a breakdown of common AI request examples:"}),"\n",(0,t.jsxs)(i.ul,{children:["\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.a,{href:"/docs/client-libraries/go/request-db/ping",children:"Ping"})}),"\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.a,{href:"/docs/client-libraries/go/request-db/info-server",children:"Info Server"})}),"\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.a,{href:"/docs/client-libraries/go/request-db/list-stores",children:"List Stores"})}),"\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.a,{href:"/docs/client-libraries/go/request-db/create-store",children:"Create Store"})}),"\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.a,{href:"/docs/client-libraries/go/request-db/set",children:"Set"})}),"\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.a,{href:"/docs/client-libraries/go/request-db/get-simn",children:"GetSimN"})}),"\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.a,{href:"/docs/client-libraries/go/request-db/get-by-predicate",children:"Get By Predicate"})}),"\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.a,{href:"/docs/client-libraries/go/request-db/create-predicate-index",children:"Create Predicate Index"})}),"\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.a,{href:"/docs/client-libraries/go/request-db/drop-predicate-index",children:"Drop Predicate Index"})}),"\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.a,{href:"/docs/client-libraries/go/request-db/delete-key",children:"Delete Key"})}),"\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.a,{href:"/docs/client-libraries/go/request-db/drop-store",children:"Drop Store"})}),"\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.a,{href:"/docs/client-libraries/go/request-db/create-non-linear-algx",children:"Create Non Linear Algorithm Index"})}),"\n",(0,t.jsx)(i.li,{children:(0,t.jsx)(i.a,{href:"/docs/client-libraries/go/request-db/drop-non-linear-algx",children:"Drop Non Linear Algorithm Index"})}),"\n"]})]})}function h(e={}){const{wrapper:i}={...(0,s.R)(),...e.components};return i?(0,t.jsx)(i,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}}}]);