"use strict";(self.webpackChunkahnlich_web=self.webpackChunkahnlich_web||[]).push([[280],{28453:(e,i,n)=>{n.d(i,{R:()=>l,x:()=>o});var t=n(96540);const r={},s=t.createContext(r);function l(e){const i=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(i):{...i,...e}}),[i,e])}function o(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),t.createElement(s.Provider,{value:i},e.children)}},46656:(e,i,n)=>{n.r(i),n.d(i,{assets:()=>c,contentTitle:()=>o,default:()=>p,frontMatter:()=>l,metadata:()=>t,toc:()=>a});const t=JSON.parse('{"id":"client-libraries/rust/pipeline","title":"Pipeline","description":"Creates a new AI pipeline for batching multiple operations in the AI client. Pipelines allow you to queue multiple requests (e.g., set, getsimn, get_key) and execute them in sequence, ensuring consistent ordering and efficient handling of AI service calls. This is particularly useful for bulk processing or workflows that require multiple embeddings or queries to be executed together.","source":"@site/docs/client-libraries/rust/pipeline.md","sourceDirName":"client-libraries/rust","slug":"/client-libraries/rust/pipeline","permalink":"/docs/client-libraries/rust/pipeline","draft":false,"unlisted":false,"editUrl":"https://github.com/deven96/ahnlich/tree/main/web/ahnlich-web/docs/client-libraries/rust/pipeline.md","tags":[],"version":"current","frontMatter":{"title":"Pipeline"},"sidebar":"docsSidebar","previous":{"title":"Purge Stores","permalink":"/docs/client-libraries/rust/request-ai/purge-stores"},"next":{"title":"Type & Utilities","permalink":"/docs/client-libraries/rust/types-and-utilities"}}');var r=n(74848),s=n(28453);const l={title:"Pipeline"},o="Pipeline",c={},a=[{value:"Source Code Example",id:"source-code-example",level:2},{value:"Parameters",id:"parameters",level:2},{value:"Returns",id:"returns",level:2},{value:"Behavior (explains the code, brief)",id:"behavior-explains-the-code-brief",level:2}];function d(e){const i={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components},{Details:n}=i;return n||function(e,i){throw new Error("Expected "+(i?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(i.header,{children:(0,r.jsx)(i.h1,{id:"pipeline",children:"Pipeline"})}),"\n",(0,r.jsxs)(i.p,{children:["Creates a new ",(0,r.jsx)(i.strong,{children:"AI pipeline"})," for batching multiple operations in the ",(0,r.jsx)(i.strong,{children:"AI client"}),". Pipelines allow you to queue multiple requests (e.g., ",(0,r.jsx)(i.code,{children:"set"}),", ",(0,r.jsx)(i.code,{children:"get_sim_n"}),", ",(0,r.jsx)(i.code,{children:"get_key"}),") and execute them in sequence, ensuring consistent ordering and efficient handling of AI service calls. This is particularly useful for bulk processing or workflows that require multiple embeddings or queries to be executed together."]}),"\n",(0,r.jsx)(i.h2,{id:"source-code-example",children:"Source Code Example"}),"\n",(0,r.jsxs)(n,{children:[(0,r.jsx)("summary",{children:"Click to expand"}),(0,r.jsx)(i.pre,{children:(0,r.jsx)(i.code,{className:"language-rust",children:'use ahnlich_client_rs::ai::{AiClient, AiPipeline};\nuse ahnlich_client_rs::error::AhnlichError;\nuse tokio;\n\n\n#[tokio::main]\nasync fn main() -> Result<(), AhnlichError> {\n    // Replace with your AI server address\n    let addr = "127.0.0.1:1370".to_string();\n\n\n    // Initialize the client\n    let client = AiClient::new(addr).await?;\n\n\n    // Create a pipeline\n    let mut pipeline = client.pipeline(None);\n\n\n    // Add some example queries\n    pipeline.ping();      // Ping the server\n    pipeline.list_stores(); // List existing stores\n\n\n    // Execute the pipeline\n    let response = pipeline.exec().await?;\n\n\n    println!("Pipeline response: {:#?}", response);\n\n\n    Ok(())\n}\n'})})]}),"\n",(0,r.jsx)(i.h2,{id:"parameters",children:"Parameters"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"tracing_id: Option<String>"})," \u2014 Optional trace parent ID for observability and distributed tracing."]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"returns",children:"Returns"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:[(0,r.jsx)(i.code,{children:"AiPipeline"})," \u2014 A new pipeline instance that can queue AI client operations and execute them sequentially."]}),"\n"]}),"\n",(0,r.jsx)(i.h2,{id:"behavior-explains-the-code-brief",children:"Behavior (explains the code, brief)"}),"\n",(0,r.jsxs)(i.ul,{children:["\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsxs)(i.p,{children:["Initializes an empty ",(0,r.jsx)(i.code,{children:"queries"})," vector to hold pending operations."]}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsx)(i.p,{children:"Clones the AI client for use within the pipeline."}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsx)(i.p,{children:"Attaches optional tracing metadata for observability."}),"\n"]}),"\n",(0,r.jsxs)(i.li,{children:["\n",(0,r.jsxs)(i.p,{children:["Returns a fully initialized ",(0,r.jsx)(i.code,{children:"AiPipeline"})," object ready for queuing operations."]}),"\n"]}),"\n"]})]})}function p(e={}){const{wrapper:i}={...(0,s.R)(),...e.components};return i?(0,r.jsx)(i,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);