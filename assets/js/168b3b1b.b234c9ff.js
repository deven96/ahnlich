"use strict";(self.webpackChunkahnlich_web=self.webpackChunkahnlich_web||[]).push([[2098],{28453:(e,n,s)=>{s.d(n,{R:()=>l,x:()=>o});var i=s(96540);const t={},r=i.createContext(t);function l(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),i.createElement(r.Provider,{value:n},e.children)}},34601:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>u,frontMatter:()=>l,metadata:()=>i,toc:()=>d});const i=JSON.parse('{"id":"client-libraries/python/bulk-requests","title":"Bulk Requests","description":"The AI client supports bulk requests, allowing you to send multiple operations at once. Instead of sending each request individually, you can batch them using a pipeline builder.","source":"@site/docs/client-libraries/python/bulk-requests.md","sourceDirName":"client-libraries/python","slug":"/client-libraries/python/bulk-requests","permalink":"/docs/client-libraries/python/bulk-requests","draft":false,"unlisted":false,"editUrl":"https://github.com/deven96/ahnlich/tree/main/web/ahnlich-web/docs/client-libraries/python/bulk-requests.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Bulk Requests","sidebar_position":4},"sidebar":"docsSidebar","previous":{"title":"Drop Non-Linear Algorithm Index","permalink":"/docs/client-libraries/python/request-ai/drop-non-linear-algx"},"next":{"title":"Type Meanings","permalink":"/docs/client-libraries/python/type-meanings"}}');var t=s(74848),r=s(28453);const l={title:"Bulk Requests",sidebar_position:4},o="Bulk Requests",c={},d=[{value:"Source Code",id:"source-code",level:2},{value:"Explanation",id:"explanation",level:2}];function a(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{Details:s}=n;return s||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"bulk-requests",children:"Bulk Requests"})}),"\n",(0,t.jsxs)(n.p,{children:["The AI client supports ",(0,t.jsx)(n.strong,{children:"bulk requests"}),", allowing you to send multiple operations at once. Instead of sending each request individually, you can batch them using a ",(0,t.jsx)(n.strong,{children:"pipeline builder"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["Bulk requests are executed ",(0,t.jsx)(n.strong,{children:"sequentially"})," in the order they were added. The client automatically collects all responses and returns them as a single aggregated result."]}),"\n",(0,t.jsx)(n.h2,{id:"source-code",children:"Source Code"}),"\n",(0,t.jsxs)(s,{children:[(0,t.jsx)("summary",{children:"Click to expand"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:'from ahnlich_client_py import AhnlichAIClient\n\nclient = AhnlichAIClient(address="127.0.0.1", port=1370)\n\n# Create a pipeline builder\nrequest_builder = client.pipeline()\n\n# Queue multiple requests\nrequest_builder.ping()\nrequest_builder.info_server()\nrequest_builder.list_clients()\nrequest_builder.list_stores()\n\n# Execute the pipeline\nresponse = client.exec()\n'})})]}),"\n",(0,t.jsx)(n.h2,{id:"explanation",children:"Explanation"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Pipeline builder"}),":\nCollects multiple requests before sending them to the AI service."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Sequential execution"}),":\nRequests are executed one after the other, preserving the order in which they were added."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Aggregated response"}),":\nThe result is a list of individual responses, one for each request in the pipeline."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Use case"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Reduce network overhead by batching requests."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Efficiently run related queries in one execution cycle."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Simplify client code when multiple calls are always needed together."}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Example responses might include:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["A ",(0,t.jsx)(n.code,{children:"Ping"})," acknowledgment."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Server info metadata."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Connected clients."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Available stores."}),"\n"]}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(a,{...e})}):a(e)}}}]);