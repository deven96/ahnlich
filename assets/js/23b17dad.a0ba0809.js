"use strict";(self.webpackChunkahnlich_web=self.webpackChunkahnlich_web||[]).push([[3655],{4373:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>i,toc:()=>a});const i=JSON.parse('{"id":"client-libraries/python/request-ai/create-store","title":"Create Store","description":"The CreateStore request is used to initialize a new AI-powered store.","source":"@site/docs/client-libraries/python/request-ai/create-store.md","sourceDirName":"client-libraries/python/request-ai","slug":"/client-libraries/python/request-ai/create-store","permalink":"/docs/client-libraries/python/request-ai/create-store","draft":false,"unlisted":false,"editUrl":"https://github.com/deven96/ahnlich/tree/main/web/ahnlich-web/docs/client-libraries/python/request-ai/create-store.md","tags":[],"version":"current","frontMatter":{"title":"Create Store"},"sidebar":"docsSidebar","previous":{"title":"List Stores","permalink":"/docs/client-libraries/python/request-ai/list-stores"},"next":{"title":"Set","permalink":"/docs/client-libraries/python/request-ai/set"}}');var t=r(74848),s=r(28453);const o={title:"Create Store"},l="Create Store",c={},a=[{value:"Key Notes",id:"key-notes",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components},{Details:r}=n;return r||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"create-store",children:"Create Store"})}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"CreateStore"})," request is used to ",(0,t.jsx)(n.strong,{children:"initialize a new AI-powered store"}),".\nUnlike the DB client (which deals with raw vector dimensions), the AI client lets you specify ",(0,t.jsx)(n.strong,{children:"pretrained AI models"})," to handle embedding generation and indexing."]}),"\n",(0,t.jsx)(n.p,{children:"This means you don\u2019t have to manage vectors manually \u2014 the AI service will automatically embed inputs using the selected models."}),"\n",(0,t.jsxs)(r,{children:[(0,t.jsx)("summary",{children:"Click to expand source code"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-py",children:'import asyncio\nfrom grpclib.client import Channel\nfrom ahnlich_client_py.grpc.services.ai_service import AiServiceStub\nfrom ahnlich_client_py.grpc.ai import query as ai_query\nfrom ahnlich_client_py.grpc.ai.models import AiModel\n\n\nasync def create_store():\n  async with Channel(host="127.0.0.1", port=1370) as channel:\n      client = AiServiceStub(channel)\n      response = await client.create_store(\n          ai_query.CreateStore(\n              store="test store",\n              query_model=AiModel.ALL_MINI_LM_L6_V2,\n              index_model=AiModel.ALL_MINI_LM_L6_V2,\n              predicates=["job"],\n              non_linear_indices=[],  # Optional: non-linear algorithms for faster search\n              error_if_exists=True,\n              # Store original controls if we choose to store the raw inputs\n              # within the DB in order to be able to retrieve the originals again\n              # during query, else only store values are returned\n              store_original=True\n          )\n      )\n      print(response) # Unit()\n\n\nif __name__ == "__main__":\n  asyncio.run(create_store())\n'})})]}),"\n",(0,t.jsx)(n.h2,{id:"key-notes",children:"Key Notes"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"query_model"})," - model used for encoding query inputs during searches."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"index_model"})," - model used for encoding stored data vectors."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"predicates"}),' - metadata fields that can be filtered against (e.g., "',(0,t.jsx)(n.code,{children:"job"}),'").']}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"non_linear_indices"})," - list of non-linear algorithms for approximate search (can be empty list)."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"store_original"})," - if ",(0,t.jsx)(n.code,{children:"True"}),", the original raw input is stored alongside embeddings for later retrieval."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Response"})," - returns ",(0,t.jsx)(n.code,{children:"Unit()"})," on success."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This request is critical in AI workflows because it allows you to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Configure ",(0,t.jsx)(n.strong,{children:"semantic stores"})," with specialized embedding models."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Decide whether to preserve raw input text/images for retrieval."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Build ",(0,t.jsx)(n.strong,{children:"intelligent"}),", ",(0,t.jsx)(n.strong,{children:"AI-driven search"})," and ",(0,t.jsx)(n.strong,{children:"recommendation systems"})," without managing embeddings manually."]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},28453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>l});var i=r(96540);const t={},s=i.createContext(t);function o(e){const n=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);