"use strict";(self.webpackChunkahnlich_web=self.webpackChunkahnlich_web||[]).push([[8130],{77735:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"simd-optimization-vector-database","metadata":{"permalink":"/blog/simd-optimization-vector-database","editUrl":"https://github.com/deven96/ahnlich/tree/main/web/ahnlich-web/blog/2026-02-14-simd-optimization-in-ahnlich.md","source":"@site/blog/2026-02-14-simd-optimization-in-ahnlich.md","title":"Making Ahnlich Faster with SIMD: A 4.7x Speedup Story","description":"Vector databases power modern search, from finding similar images to semantic document retrieval. But there\'s a hidden performance bottleneck: calculating distances between millions of high-dimensional vectors requires billions of operations per query. In this post, I\'ll show you how we used SIMD (Single Instruction, Multiple Data) to make Ahnlich 4.7x faster at these calculations.","date":"2026-02-14T00:00:00.000Z","tags":[{"inline":false,"label":"Ahnlich","permalink":"/blog/tags/ahnlich","description":"Posts about Ahnlich vector database"},{"inline":false,"label":"Performance","permalink":"/blog/tags/performance","description":"Performance benchmarks and optimization tips"},{"inline":true,"label":"simd","permalink":"/blog/tags/simd"},{"inline":true,"label":"rust","permalink":"/blog/tags/rust"}],"readingTime":12.26,"hasTruncateMarker":true,"authors":[{"name":"Diretnan Domnan","title":"Senior Systems Engineer @ Cloudflare","url":"https://github.com/deven96","page":{"permalink":"/blog/authors/diretnan"},"socials":{"x":"https://x.com/_deven96","github":"https://github.com/deven96"},"imageURL":"https://github.com/deven96.png","key":"diretnan"}],"frontMatter":{"slug":"simd-optimization-vector-database","title":"Making Ahnlich Faster with SIMD: A 4.7x Speedup Story","authors":["diretnan"],"tags":["ahnlich","performance","simd","rust"],"image":"/img/blog/simd-hero.png"},"unlisted":false},"content":"Vector databases power modern search, from finding similar images to semantic document retrieval. But there\'s a hidden performance bottleneck: calculating distances between millions of high-dimensional vectors requires billions of operations per query. In this post, I\'ll show you how we used SIMD (Single Instruction, Multiple Data) to make [Ahnlich](https://github.com/deven96/ahnlich) **4.7x faster** at these calculations.\\n\\n![SIMD Optimization Hero](/img/blog/simd-hero.svg)\\n\\n\x3c!-- truncate --\x3e\\n\\n## What\'s a Vector Database Anyway?\\n\\nMost developers are familiar with traditional databases where you filter by exact criteria. A friend recently showed me his financial market data queries that looked like this:\\n\\n```python\\nresults = db.query()\\n    .with_text_search(\\"tech stocks\\")\\n    .with_date_filter(start=\\"2024-01-01\\", end=\\"2024-12-31\\")\\n    .with_creator(\\"Goldman Sachs\\")\\n    .with_soft_deletion(False)\\n    .limit(100)\\n```\\n\\nHe was chaining together filters trying to find relevant reports. The problem? He already knew what he wanted but still had to manually specify every constraint. What if he could just search \\"recent tech stock analysis from major banks\\" and get exactly what he needed?\\n\\nThat\'s where vector databases shine.\\n\\n### Traditional Search vs Semantic Search\\n\\n**Traditional databases** match exact criteria. You tell them exactly what to look for: specific dates, specific creators, specific keywords. It\'s like searching for your friend at a concert by checking \\"wearing a red shirt AND standing near the stage AND arrived at 7pm.\\"\\n\\n**Vector databases** work with embeddings. First, your data passes through a model (like BERT, OpenAI, or others) that converts text/images into vectors (lists of numbers representing semantic meaning). The vector database then stores these embeddings and finds items that are *similar* to your query based on mathematical distance, even if the words are completely different. Some databases like Ahnlich even let you specify the embedding model as an argument, handling the conversion automatically.\\n\\nLet\'s visualize this with a simple example. Say we have three sentences converted to 2D vectors (real embeddings are 100s or 1000s of dimensions, but 2D is easier to see):\\n\\n```\\n\\"cat sleeps\\"   \u2192 [0.77, 0.23]\\n\\"dog sleeping\\" \u2192 [0.7, 0.3]  \\n\\"I ate pizza\\"  \u2192 [0.1, 0.9]\\n```\\n\\nPlotting these in 2D space:\\n\\n![Vector 2D Plot](/img/blog/vector-2d-plot.svg)\\n\\nWhen you query **\\"kitten napping\\" \u2192 [0.75, 0.25]**, the vector database calculates the distance to each stored vector and finds the closest matches. Even though \\"kitten napping\\" doesn\'t use the exact words \\"cat\\" or \\"dog,\\" the semantic meaning places it close to those vectors.\\n\\n### How Distance Calculations Work\\n\\nVector databases use mathematical distance metrics to measure similarity:\\n\\n1. **Euclidean Distance** - Straight-line distance in n-dimensional space (lower = more similar)\\n2. **Cosine Similarity** - Angle between vectors, ignoring magnitude (1 = identical direction)\\n3. **Dot Product** - Measures alignment between vectors (higher = more similar)\\n\\nLet\'s calculate Euclidean distance for our query:\\n\\n```\\nQuery: \\"kitten napping\\" [0.75, 0.25]\\n\\nDistance to \\"cat sleeps\\" [0.77, 0.23]:\\n  = sqrt((0.75-0.77)\xb2 + (0.25-0.23)\xb2) \\n  = sqrt(0.0004 + 0.0004) \\n  = 0.028 \u2713 Closest match!\\n\\nDistance to \\"dog sleeping\\" [0.7, 0.3]:\\n  = sqrt((0.75-0.7)\xb2 + (0.25-0.3)\xb2) \\n  = sqrt(0.0025 + 0.0025)\\n  = 0.071 \u2713 Also close!\\n\\nDistance to \\"I ate pizza\\" [0.1, 0.9]:\\n  = sqrt((0.75-0.1)\xb2 + (0.25-0.9)\xb2) \\n  = sqrt(0.4225 + 0.4225)\\n  = 0.92 \u2717 Far away!\\n```\\n\\nThe database returns the top results: \\"cat sleeps\\" (closest) and \\"dog sleeping\\" (second closest). Perfect semantic search!\\n\\n### The Performance Challenge\\n\\nHere\'s the catch: this toy example has 3 vectors in 2 dimensions. Real-world applications have:\\n- **Millions of vectors** (documents, images, products)\\n- **1024+ dimensions** (typical embedding size)\\n- **Thousands of queries per second**\\n\\nFor each query, you perform:\\n- **100 million vectors \xd7 1024 dimensions = 102 billion operations**\\n\\nA single search without optimization would take seconds or minutes. This is why vector databases obsess over performance, and why we need SIMD.\\n\\n## The Similarity Search Problem: Optimizing Distance Calculations\\n\\nAt the heart of every vector database query are distance calculations. Let\'s look at dot product similarity as our example:\\n\\n```rust\\n// Scalar implementation\\nfn dot_product_scalar(a: &[f32], b: &[f32]) -> f32 {\\n    a.iter()\\n        .zip(b)\\n        .map(|(&x, &y)| x * y)\\n        .sum()\\n}\\n```\\n\\nFor a 1024-dimensional vector, this performs 1024 multiplications and 1023 additions. When you\'re doing this for 100,000 vectors, that\'s over 100 million operations.\\n\\n## Enter SIMD: Parallel Processing on Steroids\\n\\nSIMD stands for Single Instruction, Multiple Data. Instead of processing one number at a time, SIMD lets your CPU process multiple numbers in a single instruction.\\n\\nThink of it like this:\\n- **Scalar**: One cashier checking out one customer at a time\\n- **SIMD**: Four cashiers working in perfect synchronization, all scanning items at the exact same moment\\n\\nModern CPUs have SIMD instruction sets like SSE, AVX, and NEON (on ARM). On an M1 Mac, NEON can process 4 x f32 values simultaneously. On Intel/AMD with AVX-512, you can process 16 x f32 values at once!\\n\\nHere\'s a visual representation:\\n\\n```\\nScalar Processing:\\n[1.0] * [2.0] = [2.0]   \u2190 One multiplication\\n[1.5] * [2.5] = [3.75]  \u2190 Another multiplication  \\n[2.0] * [3.0] = [6.0]   \u2190 Yet another...\\n[2.5] * [3.5] = [8.75]  \u2190 You get the idea\\n\\nSIMD Processing (4-wide):\\n[1.0, 1.5, 2.0, 2.5] * [2.0, 2.5, 3.0, 3.5] = [2.0, 3.75, 6.0, 8.75]\\n         \u2191 All done in ONE instruction! \u2191\\n```\\n\\n## Implementing SIMD in Ahnlich\\n\\nFor Ahnlich, we used the [`pulp`](https://crates.io/crates/pulp) crate, which provides portable SIMD abstractions across different CPU architectures. Here\'s how we implemented SIMD dot product ([see full implementation on GitHub](https://github.com/deven96/ahnlich/blob/main/ahnlich/db/src/algorithm/similarity.rs#L134-L162)):\\n\\n```rust\\nuse pulp::{Arch, Simd, WithSimd};\\n\\nstruct DotProduct<\'a> {\\n    first: &\'a [f32],\\n    second: &\'a [f32],\\n}\\n\\nimpl WithSimd for DotProduct<\'_> {\\n    type Output = f32;\\n\\n    #[inline(always)]\\n    fn with_simd<S: Simd>(self, simd: S) -> Self::Output {\\n        // Split arrays into SIMD-aligned chunks and remainder\\n        let (first_head, first_tail) = S::as_simd_f32s(self.first);\\n        let (second_head, second_tail) = S::as_simd_f32s(self.second);\\n\\n        // SIMD accumulator starting at zero\\n        let mut sum_of_points = simd.splat_f32s(0.0);\\n\\n        // Process SIMD chunks - 4 multiplies + 4 adds per iteration!\\n        for (&chunk_first, &chunk_second) in first_head.iter().zip(second_head) {\\n            sum_of_points = simd.mul_add_f32s(chunk_first, chunk_second, sum_of_points);\\n        }\\n\\n        // Reduce SIMD register to single value\\n        let mut dot_product = simd.reduce_sum_f32s(sum_of_points);\\n\\n        // Handle remaining elements with scalar code\\n        dot_product += first_tail\\n            .iter()\\n            .zip(second_tail)\\n            .map(|(&x, &y)| x * y)\\n            .sum::<f32>();\\n            \\n        dot_product\\n    }\\n}\\n\\npub fn dot_product(first: &[f32], second: &[f32]) -> f32 {\\n    let arch = Arch::new();\\n    arch.dispatch(DotProduct {\\n        first,\\n        second,\\n    })\\n}\\n```\\n\\nLet\'s break down what\'s happening:\\n\\n1. **Split into chunks**: `as_simd_f32s` splits our arrays into SIMD-aligned chunks (groups of 4 for NEON) plus a remainder tail\\n2. **SIMD accumulation**: `mul_add_f32s` performs fused multiply-add on 4 values simultaneously\\n3. **Reduction**: `reduce_sum_f32s` adds all values in the SIMD register together\\n4. **Handle remainder**: Process any leftover elements with scalar code\\n\\nThe same SIMD pattern extends to the other distance metrics Ahnlich supports. Each one benefits from parallel processing in different ways:\\n\\n### Cosine Similarity\\n\\nCosine similarity measures the angle between two vectors, making it perfect for comparing semantic meaning in text embeddings. The formula is: **dot(A, B) / (magnitude(A) \xd7 magnitude(B))**\\n\\nThe computation involves two SIMD operations: calculating the dot product (which we already saw) and computing the magnitudes. Here\'s how it looks ([source](https://github.com/deven96/ahnlich/blob/main/ahnlich/db/src/algorithm/similarity.rs#L109-L125)):\\n\\n```rust\\nfn cosine_similarity(first: &[f32], second: &[f32]) -> f32 {\\n    let dot_product = dot_product(first, second);\\n    \\n    // Calculate magnitudes using SIMD\\n    let arch = Arch::new();\\n    let magnitude = arch.dispatch(Magnitude {\\n        first,\\n        second,\\n    });\\n    \\n    dot_product / magnitude\\n}\\n```\\n\\nThe magnitude calculation also uses SIMD to compute `sqrt(\u03a3(x\xb2))` for both vectors simultaneously, giving us another layer of parallelization.\\n\\n### Euclidean Distance  \\n\\nEuclidean distance measures the straight-line distance between two points in high-dimensional space. It\'s commonly used for image embeddings and continuous numerical features. The formula is: **sqrt(\u03a3(a[i] - b[i])\xb2)**\\n\\nThis one\'s interesting because we can use SIMD for both the subtraction and the squaring ([source](https://github.com/deven96/ahnlich/blob/main/ahnlich/db/src/algorithm/similarity.rs#L198-L231)):\\n\\n```rust\\nimpl WithSimd for EuclideanDistance<\'_> {\\n    type Output = f32;\\n\\n    #[inline(always)]\\n    fn with_simd<S: Simd>(self, simd: S) -> Self::Output {\\n        let (first_head, first_tail) = S::as_simd_f32s(self.first);\\n        let (second_head, second_tail) = S::as_simd_f32s(self.second);\\n\\n        let mut sum_of_squares = simd.splat_f32s(0.0);\\n\\n        for (&cord_first, &cord_second) in first_head.iter().zip(second_head) {\\n            let diff = simd.sub_f32s(cord_first, cord_second);  // 4 subtractions\\n            sum_of_squares = simd.mul_add_f32s(diff, diff, sum_of_squares);  // 4 squares + 4 adds\\n        }\\n\\n        let mut total = simd.reduce_sum_f32s(sum_of_squares);\\n\\n        // Scalar remainder\\n        total += first_tail\\n            .iter()\\n            .zip(second_tail)\\n            .map(|(&x, &y)| {\\n                let diff = x - y;\\n                diff * diff\\n            })\\n            .sum::<f32>();\\n\\n        total.sqrt()\\n    }\\n}\\n```\\n\\nNotice how we\'re doing 8 SIMD operations per iteration (4 subtractions + 4 multiply-adds), making this particularly efficient.\\n\\n## The Benchmark Results\\n\\nLet\'s see how our SIMD implementation performs. Testing with 100,000 vectors of 1024 dimensions each on Apple M1:\\n\\n```rust\\nfn main() {\\n    let dimension = 1024;\\n    let size = 100_000;\\n    \\n    let query: Vec<f32> = (0..dimension).map(|i| i as f32 * 0.1).collect();\\n    let vectors: Vec<Vec<f32>> = (0..size)\\n        .map(|_| (0..dimension).map(|i| i as f32 * 0.01).collect())\\n        .collect();\\n    \\n    // Benchmark scalar\\n    let start = Instant::now();\\n    for v in &vectors {\\n        let _ = dot_product_scalar(&query, v);\\n    }\\n    let scalar_duration = start.elapsed();\\n    \\n    // Benchmark SIMD\\n    let start = Instant::now();\\n    for v in &vectors {\\n        let _ = dot_product_simd(&query, v);\\n    }\\n    let simd_duration = start.elapsed();\\n    \\n    println!(\\"Scalar: {:?}\\", scalar_duration);\\n    println!(\\"SIMD:   {:?}\\", simd_duration);\\n    println!(\\"Speedup: {:.2}x\\", scalar_duration.as_secs_f64() / simd_duration.as_secs_f64());\\n}\\n```\\n\\n**Results (Apple M1 with ARM NEON)**:\\n```\\nScalar duration: 111.22ms\\nSIMD duration:   23.49ms\\n\\nSpeedup: 4.73x\\n```\\n\\n4.7x faster with SIMD. For a vector database handling millions of similarity calculations per second, this translates to significant real-world performance improvements.\\n\\n## Peeking Under the Hood: The Assembly\\n\\nLet\'s verify what\'s actually happening at the assembly level. Using `cargo-show-asm`, we can inspect the generated machine code:\\n\\n```bash\\ncargo install cargo-show-asm\\ncargo asm --rust simd_bench::dot_product_simd\\n```\\n\\nKey SIMD instruction from the output:\\n```asm\\nLBB0_4:\\n    ldr q1, [x10], #16       ; Load 4 x f32 from first array into vector register\\n    ldr q2, [x11], #16       ; Load 4 x f32 from second array into vector register\\n    fmla.4s v0, v2, v1       ; Fused multiply-add: v0 += v2 * v1 (4 operations!)\\n    subs x9, x9, #1          ; Decrement counter\\n    b.ne LBB0_4              ; Loop if not done\\n```\\n\\nThe magic line is `fmla.4s v0, v2, v1`. This single ARM NEON instruction performs **4 multiply-adds simultaneously**. Instead of:\\n```\\nresult += a[0] * b[0]\\nresult += a[1] * b[1]\\nresult += a[2] * b[2]\\nresult += a[3] * b[3]\\n```\\n\\nWe get all four operations in **one instruction cycle**. That\'s the power of SIMD.\\n\\n## The Auto-Vectorization Question\\n\\nModern compilers like LLVM are surprisingly good at auto-vectorization. You might wonder, \\"Does the compiler already vectorize my scalar code?\\"\\n\\nSometimes, but not always.\\n\\nLet\'s examine what LLVM does with our scalar implementation. The scalar version uses iterator combinators:\\n\\n```rust\\nfn dot_product_scalar(a: &[f32], b: &[f32]) -> f32 {\\n    a.iter().zip(b).map(|(&x, &y)| x * y).sum()\\n}\\n```\\n\\nWhen compiled with `-C opt-level=3` (release mode), LLVM\'s auto-vectorizer kicks in and **does** generate SIMD instructions for this simple pattern. This is why our benchmark shows good scalar performance, it\'s actually using SIMD under the hood!\\n\\n### Proving Auto-Vectorization\\n\\nWe can verify this by examining the assembly or by disabling SIMD features entirely. On ARM, we can disable NEON:\\n\\n```bash\\nRUSTFLAGS=\\"-C target-feature=-neon\\" cargo run --release\\n```\\n\\n**Results without NEON**:\\n```\\nScalar duration: 762.05ms  (7x slower)\\nSIMD duration:   131.84ms  (explicit SIMD fallback path)\\n```\\n\\nThe scalar version tanks without auto-vectorization, taking **762ms** instead of **111ms**.\\n\\n### Why Explicit SIMD Still Matters\\n\\nIf compilers auto-vectorize, why write explicit SIMD code?\\n\\n1. **Compiler Limitations**: Auto-vectorization works well for simple loops but fails on complex patterns. Vector databases have intricate algorithms (graph traversal, tree searches) where manual SIMD wins.\\n\\n2. **Portability Guarantees**: Auto-vectorization quality varies across:\\n   - Different compilers (GCC vs Clang vs MSVC)\\n   - Compiler versions (what works in LLVM 15 may not in LLVM 12)\\n   - Target architectures (x86 vs ARM vs RISC-V)\\n   \\n   Explicit SIMD using `pulp` ensures consistent performance everywhere.\\n\\n3. **Control Over Optimization**: Sometimes you need fine-grained control:\\n   - Custom reduction operations\\n   - Specific instruction selection (FMA over separate mul+add)\\n   - Memory alignment requirements\\n   \\n4. **Performance Predictability**: Auto-vectorization is \\"best effort.\\" A small code change can break vectorization without warning. Explicit SIMD gives predictable performance.\\n\\n5. **Documentation**: Explicit SIMD clearly communicates intent. Future maintainers know this code *must* be fast.\\n\\n### When Auto-Vectorization Fails\\n\\nHere\'s a real example where auto-vectorization struggles:\\n\\n```rust\\n// Compiler probably vectorizes this \u2705\\nfn simple_sum(arr: &[f32]) -> f32 {\\n    arr.iter().sum()\\n}\\n\\n// Compiler struggles to vectorize this \u274c\\nfn conditional_sum(arr: &[f32], threshold: f32) -> f32 {\\n    let mut sum = 0.0;\\n    for &val in arr {\\n        if val > threshold {\\n            sum += val * val;\\n        } else {\\n            sum += val;\\n        }\\n    }\\n    sum\\n}\\n```\\n\\nBranches inside hot loops confuse auto-vectorizers. SIMD techniques like masking or blend instructions can handle this, but you need explicit SIMD to access them.\\n\\n### Visualizing Auto-Vectorization\\n\\nHere\'s what happens at compile time:\\n\\n![Auto-vectorization Flow](/img/blog/auto-vectorization-flow.svg)\\n\\nWith explicit SIMD using `pulp`, we bypass these heuristics and tell the compiler exactly what to do.\\n\\n## The Real-World Impact\\n\\nWhile our simple dot product didn\'t show dramatic speedups on M1 due to auto-vectorization, the benefits become clear in:\\n\\n- **Cross-platform consistency**: The SIMD code performs predictably across ARM, x86, and other architectures\\n- **Complex algorithms**: KD-tree and HNSW implementations benefit from hand-tuned SIMD\\n- **Future-proofing**: As we add more sophisticated distance metrics, explicit SIMD control helps maintain performance\\n\\nHere\'s our full suite of similarity algorithms using SIMD:\\n\\n| Algorithm | Use Case | SIMD Benefit |\\n|-----------|----------|--------------|\\n| Cosine Similarity | Text embeddings, semantic search | Magnitude + dot product SIMD |\\n| Euclidean Distance | Image vectors, continuous features | Fused multiply-add for differences |\\n| Dot Product | Recommendation systems | Direct SIMD multiplication |\\n\\n## Key Takeaways\\n\\n1. **SIMD provides real speedups**: We achieved 4.7x faster dot product calculations on ARM NEON\\n2. **Modern compilers auto-vectorize simple patterns**: LLVM turned our scalar code into SIMD instructions automatically\\n3. **Auto-vectorization has limits**: Complex algorithms, branches, and intricate patterns need explicit SIMD\\n4. **Explicit SIMD guarantees performance**: No surprises across compilers, versions, or architectures\\n5. **The `pulp` crate rocks**: Portable SIMD in Rust without drowning in architecture-specific intrinsics\\n6. **Always verify with assembly**: Use `cargo-show-asm` to see what\'s really happening\\n7. **Benchmark on real hardware**: Measure performance on your actual target architecture\\n\\n## Epilogue: What About Linux/x86?\\n\\nThe benchmarks shown here are on Apple M1 (ARM NEON). On Intel/AMD with AVX2 or AVX-512, the SIMD width is larger (8 to 16 x f32 vs 4 x f32), which means:\\n- More parallelism per instruction\\n- Different auto-vectorization behavior  \\n- Potentially larger SIMD vs scalar gaps\\n\\nIf you\'re running Ahnlich on x86, I\'d love to see your benchmark results! Open an issue or PR on [GitHub](https://github.com/deven96/ahnlich) with your numbers.\\n\\n## References\\n\\n- [Ahnlich GitHub Repository](https://github.com/deven96/ahnlich)\\n- [Pulp: Portable SIMD for Rust](https://crates.io/crates/pulp)\\n- [LLVM Auto-Vectorization Documentation](https://llvm.org/docs/Vectorizers.html)\\n- [Understanding LLVM\'s Loop Vectorizer](https://llvm.org/devmtg/2013-04/achalmers-slides.pdf)\\n- [ARM NEON Intrinsics Guide](https://developer.arm.com/architectures/instruction-sets/simd-isas/neon)\\n- [Intel AVX Intrinsics Reference](https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html)\\n- [Cargo Show ASM Tool](https://github.com/pacak/cargo-show-asm)\\n- [Rust Performance Book - SIMD](https://nnethercote.github.io/perf-book/simd.html)\\n\\n---\\n\\n_Want to try Ahnlich? Check out our [documentation](https://ahnlich.rs) and join our community. Contributions welcome!_"}]}}')}}]);