"use strict";(self.webpackChunkahnlich_web=self.webpackChunkahnlich_web||[]).push([[9863],{28453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>l});var t=i(96540);const d={},s=t.createContext(d);function r(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(d):e.components||d:r(e.components),t.createElement(s.Provider,{value:n},e.children)}},35924:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>r,metadata:()=>t,toc:()=>a});const t=JSON.parse('{"id":"components/ahnlich-ai/advanced","title":"Advanced","description":"Unlike Ahnlich DB, which is concerned with similarity algorithms and indexing, Ahnlich AI focuses on embedding generation. The service introduces model-aware stores, where you define the embedding models used for both data insertion (indexing) and querying. This abstraction lets developers work directly with raw inputs (text or images) while the AI proxy handles embedding generation.","source":"@site/docs/components/ahnlich-ai/advanced.md","sourceDirName":"components/ahnlich-ai","slug":"/components/ahnlich-ai/advanced","permalink":"/docs/components/ahnlich-ai/advanced","draft":false,"unlisted":false,"editUrl":"https://github.com/deven96/ahnlich/tree/main/web/ahnlich-web/docs/components/ahnlich-ai/advanced.md","tags":[],"version":"current","sidebarPosition":30,"frontMatter":{"title":"Advanced","sidebar_position":30},"sidebar":"docsSidebar","previous":{"title":"Reference","permalink":"/docs/components/ahnlich-ai/reference"},"next":{"title":"Deeper Dive","permalink":"/docs/components/ahnlich-ai/deep-dive"}}');var d=i(74848),s=i(28453);const r={title:"Advanced",sidebar_position:30},l="Advanced",c={},a=[{value:"Supported Models",id:"supported-models",level:2},{value:"Supported Input Types",id:"supported-input-types",level:2},{value:"Example \u2013 Creating a Model-Aware Store",id:"example--creating-a-model-aware-store",level:2},{value:"Choosing the Right Model",id:"choosing-the-right-model",level:2}];function o(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,d.jsxs)(d.Fragment,{children:[(0,d.jsx)(n.header,{children:(0,d.jsx)(n.h1,{id:"advanced",children:"Advanced"})}),"\n",(0,d.jsxs)(n.p,{children:["Unlike Ahnlich DB, which is concerned with similarity algorithms and indexing, ",(0,d.jsx)(n.strong,{children:"Ahnlich AI focuses on embedding generation"}),". The service introduces ",(0,d.jsx)(n.strong,{children:"model-aware stores"}),", where you define the embedding models used for both data insertion (indexing) and querying. This abstraction lets developers work directly with raw inputs (text or images) while the AI proxy handles embedding generation."]}),"\n",(0,d.jsx)(n.h2,{id:"supported-models",children:"Supported Models"}),"\n",(0,d.jsxs)(n.p,{children:["Ahnlich AI includes several pre-trained models that can be configured depending on your workload. These cover both ",(0,d.jsx)(n.strong,{children:"text embeddings"})," and ",(0,d.jsx)(n.strong,{children:"image embeddings"}),":"]}),"\n",(0,d.jsxs)(n.table,{children:[(0,d.jsx)(n.thead,{children:(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.th,{children:"Model Name"}),(0,d.jsx)(n.th,{children:"String Name"}),(0,d.jsx)(n.th,{children:"Type"}),(0,d.jsx)(n.th,{children:"Description"})]})}),(0,d.jsxs)(n.tbody,{children:[(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"ALL_MINI_LM_L6_V2"}),(0,d.jsx)(n.td,{children:"all-minilm-l6-v2"}),(0,d.jsx)(n.td,{children:"Text"}),(0,d.jsx)(n.td,{children:"Lightweight sentence transformer. Fast and memory-efficient, ideal for semantic similarity in applications like FAQ search or chatbots."})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"ALL_MINI_LM_L12_V2"}),(0,d.jsx)(n.td,{children:"all-minilm-l12-v2"}),(0,d.jsx)(n.td,{children:"Text"}),(0,d.jsx)(n.td,{children:"Larger variant of MiniLM. Higher accuracy for nuanced text similarity tasks, but with increased compute requirements."})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"BGE_BASE_EN_V15"}),(0,d.jsx)(n.td,{children:"bge-base-en-v1.5"}),(0,d.jsx)(n.td,{children:"Text"}),(0,d.jsx)(n.td,{children:"Base version of the BGE (English v1.5) model. Balanced performance and speed, suitable for production-scale applications."})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"BGE_LARGE_EN_V15"}),(0,d.jsx)(n.td,{children:"bge-large-en-v1.5"}),(0,d.jsx)(n.td,{children:"Text"}),(0,d.jsx)(n.td,{children:"High-accuracy embedding model for semantic search and retrieval. Best choice when precision is more important than latency."})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"RESNET50"}),(0,d.jsx)(n.td,{children:"resnet-50"}),(0,d.jsx)(n.td,{children:"Image"}),(0,d.jsx)(n.td,{children:"Convolutional Neural Network (CNN) for extracting embeddings from images. Useful for content-based image retrieval and clustering."})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"CLIP_VIT_B32_IMAGE"}),(0,d.jsx)(n.td,{children:"clip-vit-b32-image"}),(0,d.jsx)(n.td,{children:"Image"}),(0,d.jsx)(n.td,{children:"Vision Transformer encoder from the CLIP model. Produces embeddings aligned with its paired text encoder for multimodal tasks."})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"CLIP_VIT_B32_TEXT"}),(0,d.jsx)(n.td,{children:"clip-vit-b32-text"}),(0,d.jsx)(n.td,{children:"Text"}),(0,d.jsx)(n.td,{children:"Text encoder from CLIP. Designed to map textual inputs into the same space as CLIP image embeddings for text-to-image or image-to-text search."})]})]})]}),"\n",(0,d.jsx)(n.h2,{id:"supported-input-types",children:"Supported Input Types"}),"\n",(0,d.jsxs)(n.table,{children:[(0,d.jsx)(n.thead,{children:(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.th,{children:"Input Type"}),(0,d.jsx)(n.th,{children:"Description"})]})}),(0,d.jsxs)(n.tbody,{children:[(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"RAW_STRING"}),(0,d.jsx)(n.td,{children:"Accepts natural text (sentences, paragraphs). Transformed into embeddings via a selected text-based model."})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"IMAGE"}),(0,d.jsx)(n.td,{children:"Accepts image files as input. Converted into embeddings via a selected image-based model (e.g., ResNet or CLIP)."})]})]})]}),"\n",(0,d.jsx)(n.h2,{id:"example--creating-a-model-aware-store",children:"Example \u2013 Creating a Model-Aware Store"}),"\n",(0,d.jsx)(n.pre,{children:(0,d.jsx)(n.code,{children:"CREATESTORE my_store QUERYMODEL all-minilm-l6-v2 INDEXMODEL all-minilm-l6-v2\n"})}),"\n",(0,d.jsxs)(n.ul,{children:["\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsxs)(n.p,{children:[(0,d.jsx)(n.strong,{children:"index_model"})," - defines how inserted data is embedded before being stored in Ahnlich DB."]}),"\n"]}),"\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsxs)(n.p,{children:[(0,d.jsx)(n.strong,{children:"query_model"})," - defines how queries are embedded at search time."]}),"\n"]}),"\n",(0,d.jsxs)(n.li,{children:["\n",(0,d.jsxs)(n.p,{children:["Both models must output embeddings of the ",(0,d.jsx)(n.strong,{children:"same dimensionality"})," to ensure compatibility."]}),"\n"]}),"\n"]}),"\n",(0,d.jsx)(n.h2,{id:"choosing-the-right-model",children:"Choosing the Right Model"}),"\n",(0,d.jsxs)(n.table,{children:[(0,d.jsx)(n.thead,{children:(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.th,{children:"Model"}),(0,d.jsx)(n.th,{children:"Best Use Case"})]})}),(0,d.jsxs)(n.tbody,{children:[(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"MiniLM (L6/L12)"}),(0,d.jsx)(n.td,{children:"Fast, efficient semantic similarity (FAQs, chatbots)."})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"BGE (Base/Large)"}),(0,d.jsx)(n.td,{children:"High semantic accuracy for production-scale applications."})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"ResNet50"}),(0,d.jsx)(n.td,{children:"Image-to-image similarity and clustering."})]}),(0,d.jsxs)(n.tr,{children:[(0,d.jsx)(n.td,{children:"CLIP (Text+Image)"}),(0,d.jsx)(n.td,{children:"Multimodal retrieval (text-to-image / image-to-text search)."})]})]})]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,d.jsx)(n,{...e,children:(0,d.jsx)(o,{...e})}):o(e)}}}]);