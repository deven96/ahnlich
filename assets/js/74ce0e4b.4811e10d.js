"use strict";(self.webpackChunkahnlich_web=self.webpackChunkahnlich_web||[]).push([[3361],{28453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var s=i(96540);const t={},r=s.createContext(t);function a(e){const n=s.useContext(r);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),s.createElement(r.Provider,{value:n},e.children)}},38650:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"components/ahnlich-ai/use-cases","title":"Use Cases","description":"1. Semantic Search with Raw Input","source":"@site/docs/components/ahnlich-ai/use-cases.md","sourceDirName":"components/ahnlich-ai","slug":"/components/ahnlich-ai/use-cases","permalink":"/docs/components/ahnlich-ai/use-cases","draft":false,"unlisted":false,"editUrl":"https://github.com/deven96/ahnlich/tree/main/web/ahnlich-web/docs/components/ahnlich-ai/use-cases.md","tags":[],"version":"current","frontMatter":{"title":"Use Cases"},"sidebar":"docsSidebar","previous":{"title":"\ud83e\udd16 Ahnlich AI","permalink":"/docs/components/ahnlich-ai/"},"next":{"title":"Setup & Configuration","permalink":"/docs/components/ahnlich-ai/setup-config"}}');var t=i(74848),r=i(28453);const a={title:"Use Cases"},o="Use Cases",l={},c=[{value:"1. Semantic Search with Raw Input",id:"1-semantic-search-with-raw-input",level:2},{value:"2. Cross-Modal Search (Text \u2194 Image)",id:"2-cross-modal-search-text--image",level:2},{value:"3. Personalized Recommendations",id:"3-personalized-recommendations",level:2},{value:"4. Multimodal Applications in Healthcare",id:"4-multimodal-applications-in-healthcare",level:2},{value:"Pattern A  Two Stores with Metadata Linking",id:"pattern-a--two-stores-with-metadata-linking",level:3},{value:"Pattern B \u2014 Cross-Modal Matching with CLIP",id:"pattern-b--cross-modal-matching-with-clip",level:3},{value:"5. Real-Time Assistance",id:"5-real-time-assistance",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{Details:i}=n;return i||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"use-cases",children:"Use Cases"})}),"\n",(0,t.jsx)(n.h2,{id:"1-semantic-search-with-raw-input",children:"1. Semantic Search with Raw Input"}),"\n",(0,t.jsxs)(n.p,{children:["A news website integrates Ahnlich AI with ",(0,t.jsx)(n.code,{children:"all-minilm-l6-v2"}),". Instead of keyword search, users type:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'SEARCH "climate change and food security" IN news_store WHERE (topic != sports)\n'})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"The text query is embedded automatically."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Stored articles are embedded consistently."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Ahnlich DB returns top semantic matches, filtering irrelevant categories."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["This provides ",(0,t.jsx)(n.strong,{children:"conceptual search"})," rather than exact word matching."]}),"\n",(0,t.jsx)(n.h2,{id:"2-cross-modal-search-text--image",children:"2. Cross-Modal Search (Text \u2194 Image)"}),"\n",(0,t.jsx)(n.p,{children:"A fashion platform configures:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Index Model = ",(0,t.jsx)(n.code,{children:"resnet-50"})," (for product images)"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Query Model = ",(0,t.jsx)(n.code,{children:"all-minilm-l6-v2"})," (for user text queries)"]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"When a user searches for a product:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"GETSIMN 5 WITH [red summer dress] USING cosinesimilarity IN fashion_store\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Ahnlich AI embeds the ",(0,t.jsx)(n.strong,{children:"text query"})," and compares it against ",(0,t.jsx)(n.strong,{children:"image embeddings"})," in the store. This allows retrieving visually similar dresses from the catalog \u2014 without the store owner needing to manually tag the images."]}),"\n",(0,t.jsx)(n.h2,{id:"3-personalized-recommendations",children:"3. Personalized Recommendations"}),"\n",(0,t.jsxs)(n.p,{children:["Ahnlich AI can also transform ",(0,t.jsx)(n.strong,{children:"user profiles"})," or ",(0,t.jsx)(n.strong,{children:"behaviors into embeddings"})," automatically."]}),"\n",(0,t.jsxs)(n.p,{children:["Example: an e-commerce platform using ",(0,t.jsx)(n.code,{children:"product_store"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"GETSIMN 5 WITH [eco-friendly home products] USING cosinesimilarity IN product_store WHERE (status = in_stock)\n"})}),"\n",(0,t.jsx)(n.p,{children:"Here:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.strong,{children:"user query"})," is embedded."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["It\u2019s matched against ",(0,t.jsx)(n.strong,{children:"product embeddings"}),"."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["The results are filtered using the ",(0,t.jsx)(n.strong,{children:"predicate"})," ",(0,t.jsx)(n.code,{children:"(status = in_stock)"}),"."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["This enables ",(0,t.jsx)(n.strong,{children:"real-time, personalized product recommendations"})," tailored to availability."]}),"\n",(0,t.jsx)(n.h2,{id:"4-multimodal-applications-in-healthcare",children:"4. Multimodal Applications in Healthcare"}),"\n",(0,t.jsx)(n.p,{children:"Ahnlich AI does not support mixing image and text embeddings in a single store. Each store is model-aware and tied to one input type (text or image)."}),"\n",(0,t.jsxs)(n.p,{children:["For workflows such as CT scans and radiology reports, the recommended approach is to create ",(0,t.jsx)(n.strong,{children:"two separate stores"})," and link them using metadata fields like ",(0,t.jsx)(n.code,{children:"patient_id"})," or ",(0,t.jsx)(n.code,{children:"report_id"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"pattern-a--two-stores-with-metadata-linking",children:"Pattern A  Two Stores with Metadata Linking"}),"\n",(0,t.jsxs)(i,{children:[(0,t.jsx)("summary",{children:"Click to expand"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'create an image store\nCREATESTORE ct_image_store QUERYMODEL resnet-50 INDEXMODEL resnet-50 STOREORIGINAL;\nCREATEPREDINDEX (patient_id, report_id) IN ct_image_store;\n\ncreate a text report store\nCREATESTORE report_store QUERYMODEL all-minilm-l6-v2 INDEXMODEL all-minilm-l6-v2 STOREORIGINAL;\nCREATEPREDINDEX (patient_id, report_id) IN report_store;\n\ninsert CT scan\nSET (([<image-vector>], {patient_id: "P123", report_id: "R789"})) IN ct_image_store;\n\ninsert report\nSET ((["Findings: ground-glass opacities ..."], {patient_id: "P123", report_id: "R789"})) IN report_store;\n\nquery CT scans\nGETSIMN 5 WITH [<image-vector>] USING cosinesimilarity IN ct_image_store;\nfetch linked report\nGETPRED (report_id = "R789") IN report_store;\n'})})]}),"\n",(0,t.jsxs)(n.p,{children:["Here, similarity search on ",(0,t.jsx)(n.code,{children:"ct_image_store"})," finds related scans, and ",(0,t.jsx)(n.code,{children:"report_id"})," links results to the associated report in ",(0,t.jsx)(n.code,{children:"report_store"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"pattern-b--cross-modal-matching-with-clip",children:"Pattern B \u2014 Cross-Modal Matching with CLIP"}),"\n",(0,t.jsxs)(i,{children:[(0,t.jsx)("summary",{children:"Click to expand"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'-- create a cross-modal store (text \u2194 image)\nCREATESTORE clip_image_store QUERYMODEL CLIP_VIT_B32_TEXT INDEXMODEL CLIP_VIT_B32_IMAGE STOREORIGINAL;\nCREATEPREDINDEX (patient_id, report_id) IN clip_image_store;\n\ninsert CT scan\nSET (([<image-vector>], {patient_id: "P123", report_id: "R789"})) IN clip_image_store;\n\nquery with text (embedded via CLIP text model)\nGETSIMN 5 WITH ["ground-glass opacity in left lung"] USING cosinesimilarity IN clip_image_store\n'})})]}),"\n",(0,t.jsx)(n.h2,{id:"5-real-time-assistance",children:"5. Real-Time Assistance"}),"\n",(0,t.jsx)(n.p,{children:"A chatbot connected to Ahnlich AI can provide real-time recommendations by retrieving similar past support tickets."}),"\n",(0,t.jsx)(n.p,{children:"Example flow:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["The user submits a query:\n",(0,t.jsx)(n.code,{children:'"I need help with my billing issue"'})]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"The query is converted into embeddings automatically."}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Retrieve the top N similar past tickets using ",(0,t.jsx)(n.code,{children:"GETSIMN"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'GETSIMN 5 WITH ["I need help with my billing issue"] USING cosinesimilarity IN support_store\n'})}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["Optionally, filter results using metadata with ",(0,t.jsx)(n.code,{children:"GETPRED"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"GETPRED (resolved = true) IN support_store\n"})}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This workflow allows the chatbot to return relevant historical solutions with low latency, using similarity search combined with predicate filtering."})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);