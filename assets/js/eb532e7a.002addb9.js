"use strict";(self.webpackChunkahnlich_web=self.webpackChunkahnlich_web||[]).push([[9013],{28453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>o});var s=i(96540);const t={},l=s.createContext(t);function r(e){const n=s.useContext(l);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(l.Provider,{value:n},e.children)}},38137:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>c,frontMatter:()=>r,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"components/ahnlich-ai/ahnlich-ai","title":"\ud83e\udd16 Ahnlich AI","description":"Overview","source":"@site/docs/components/ahnlich-ai/ahnlich-ai.md","sourceDirName":"components/ahnlich-ai","slug":"/components/ahnlich-ai/","permalink":"/docs/components/ahnlich-ai/","draft":false,"unlisted":false,"editUrl":"https://github.com/deven96/ahnlich/tree/main/web/ahnlich-web/docs/components/ahnlich-ai/ahnlich-ai.md","tags":[],"version":"current","sidebarPosition":30,"frontMatter":{"title":"\ud83e\udd16 Ahnlich AI","sidebar_position":30},"sidebar":"docsSidebar","previous":{"title":"Advanced","permalink":"/docs/components/ahnlich-db/advanced"},"next":{"title":"Use Cases","permalink":"/docs/components/ahnlich-ai/use-cases"}}');var t=i(74848),l=i(28453);const r={title:"\ud83e\udd16 Ahnlich AI",sidebar_position:30},o="Ahnlich AI",a={},d=[{value:"Overview",id:"overview",level:2},{value:"Ahnlich AI introduces the concept of model-aware stores.",id:"ahnlich-ai-introduces-the-concept-of-model-aware-stores",level:3},{value:"Example \u2013 Creating a Store:",id:"example--creating-a-store",level:3},{value:"1. Raw Input to Embeddings",id:"1-raw-input-to-embeddings",level:2},{value:"Text Input Example:",id:"text-input-example",level:3},{value:"2. Model-Aware Stores",id:"2-model-aware-stores",level:2},{value:"Unimodal Example (Text-to-Text)",id:"unimodal-example-text-to-text",level:3},{value:"Cross-Modal Example (Text-to-Image)",id:"cross-modal-example-text-to-image",level:3},{value:"3. Off-the-Shelf Models",id:"3-off-the-shelf-models",level:2},{value:"4. Natural Querying",id:"4-natural-querying",level:2},{value:"Example \u2013 Querying with Text:",id:"example--querying-with-text",level:3}];function h(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,l.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"ahnlich-ai",children:"Ahnlich AI"})}),"\n",(0,t.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,t.jsxs)(n.p,{children:["Ahnlich AI (",(0,t.jsx)(n.code,{children:"ahnlich-ai"}),") is the ",(0,t.jsx)(n.strong,{children:"AI proxy layer"})," for Ahnlich DB, designed to make working with embeddings ",(0,t.jsx)(n.strong,{children:"effortless and intuitive"}),". While Ahnlich DB specializes in fast, in-memory storage and similarity search of vector embeddings, it expects developers to provide embeddings themselves. Ahnlich AI solves this problem by ",(0,t.jsx)(n.strong,{children:"handling the embedding generation pipeline automatically"}),"."]}),"\n",(0,t.jsxs)(n.p,{children:["Instead of manually computing vectors with external libraries and then inserting them into the database, developers can work with ",(0,t.jsx)(n.strong,{children:"raw inputs"})," such as text, images, or other modalities. Ahnlich AI transforms those inputs into embeddings using ",(0,t.jsx)(n.strong,{children:"off-the-shelf machine learning models"}),", stores them into the right vector store, and later applies the same transformation logic when queries are made."]}),"\n",(0,t.jsxs)(n.p,{children:["This design allows developers and engineers to focus on solving ",(0,t.jsx)(n.strong,{children:"application-level problems"})," such as building semantic search, recommendation engines, multimodal systems, or intelligent assistants without worrying about the complexity of embedding generation, model integration, or consistency between queries and stored data."]}),"\n",(0,t.jsx)(n.h3,{id:"ahnlich-ai-introduces-the-concept-of-model-aware-stores",children:"Ahnlich AI introduces the concept of model-aware stores."}),"\n",(0,t.jsx)(n.p,{children:"When creating a store, you must specify:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Index Model"})," \u2013 used when inserting new data into the store. Each input (text, image, etc.) is transformed into a vector embedding with this model before being stored."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Query Model"})," \u2013 used when searching the store. Each query input is transformed with this model to ensure results are compared in the same semantic space."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Constraint"})," \u2013 both the index model and the query model must produce embeddings of the ",(0,t.jsx)(n.strong,{children:"same dimensionality"}),". This ensures compatibility between stored vectors and query vectors, allowing accurate similarity comparisons."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["This separation allows you to configure workflows for ",(0,t.jsx)(n.strong,{children:"different modalities"})," or even ",(0,t.jsx)(n.strong,{children:"cross-modal retrieval"}),". For example:"]}),"\n",(0,t.jsx)(n.p,{children:"This separation allows you to configure workflows for different use cases while keeping embeddings aligned in the same dimensional space. For example:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["A store that indexes and queries with ",(0,t.jsx)(n.strong,{children:"all-minilm-l6-v2"})," for semantic text search."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["A store that uses two compatible models with the ",(0,t.jsx)(n.strong,{children:"same embedding dimensions"}),", where one is optimized for indexing documents and the other for handling queries."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"example--creating-a-store",children:"Example \u2013 Creating a Store:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"CREATESTORE my_store QUERYMODEL all-minilm-l6-v2 INDEXMODEL all-minilm-l6-v2\n"})}),"\n",(0,t.jsx)(n.p,{children:"Rust API equivalent:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-rust",children:'  create_store(\n      store="my_store",\n      index_model="all-minilm-l6-v2",\n      query_model="all-minilm-l6-v2",\n  )\n'})}),"\n",(0,t.jsxs)(n.p,{children:["This ensures that whenever you insert or query ",(0,t.jsx)(n.code,{children:"my_store"}),", Ahnlich AI automatically applies the right embedding model under the hood."]}),"\n",(0,t.jsx)(n.h2,{id:"1-raw-input-to-embeddings",children:"1. Raw Input to Embeddings"}),"\n",(0,t.jsx)(n.p,{children:"Traditionally, developers relied on external libraries (e.g., PyTorch, HuggingFace) to generate embeddings before pushing them into a database. With Ahnlich AI, this step can be handled directly within the proxy using off-the-shelf models, simplifying the workflow."}),"\n",(0,t.jsx)(n.h3,{id:"text-input-example",children:"Text Input Example:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'INSERT "The rise of renewable energy storage solutions" INTO article_store\n'})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Ahnlich AI transforms the sentence into a vector embedding (e.g., [0.12, -0.34, 0.91, ...]), then sends it to Ahnlich DB for storage."}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"2-model-aware-stores",children:"2. Model-Aware Stores"}),"\n",(0,t.jsxs)(n.p,{children:["Because embeddings depend heavily on which model is used, Ahnlich AI makes stores ",(0,t.jsx)(n.strong,{children:"model-aware"}),"."]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["If a store is configured with a ",(0,t.jsx)(n.strong,{children:"text model"}),", both data and queries are handled as text embeddings."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["If configured with a ",(0,t.jsx)(n.strong,{children:"text-to-image setup"}),", one model handles indexing images and another handles queries in text."]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["This flexibility allows ",(0,t.jsx)(n.strong,{children:"multimodal workflows"}),", where developers don\u2019t need to manually align embeddings."]}),"\n",(0,t.jsx)(n.h3,{id:"unimodal-example-text-to-text",children:"Unimodal Example (Text-to-Text)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"CREATESTORE product_store QUERYMODEL all-minilm-l6-v2 INDEXMODEL all-minilm-l6-v2\n"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["At insert time, product data (e.g., descriptions, attributes) is embedded using the index model (",(0,t.jsx)(n.code,{children:"all-minilm-l6-v2"}),")."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["At query time, natural text inputs like ",(0,t.jsx)(n.em,{children:"\u201cblue denim jacket\u201d"})," are also embedded using the query model (",(0,t.jsx)(n.code,{children:"all-minilm-l6-v2"}),")."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Since both use the same model and dimensions, embeddings exist in the same semantic space, making similarity search possible."}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"cross-modal-example-text-to-image",children:"Cross-Modal Example (Text-to-Image)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"CREATESTORE image_store QUERYMODEL all-minilm-l6-v2 INDEXMODEL resnet-50\n"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["At insert time, product images are embedded using the index model (",(0,t.jsx)(n.code,{children:"resnet-50"}),")."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:["At query time, text queries like ",(0,t.jsx)(n.em,{children:"\u201cblue denim jacket\u201d"})," are embedded using the query model (",(0,t.jsx)(n.code,{children:"all-minilm-l6-v2"}),")."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:"Because Ahnlich aligns embeddings across modalities, the system can retrieve images relevant to the text query."}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["This ensures that Ahnlich AI can support both ",(0,t.jsx)(n.strong,{children:"unimodal (text-text, image-image)"})," and ",(0,t.jsx)(n.strong,{children:"cross-modal (text-image, image-text)"})," scenarios effectively."]}),"\n",(0,t.jsx)(n.h2,{id:"3-off-the-shelf-models",children:"3. Off-the-Shelf Models"}),"\n",(0,t.jsxs)(n.p,{children:["Ahnlich AI comes with several ",(0,t.jsx)(n.strong,{children:"pre-integrated models"})," that work out of the box for text and image similarity tasks."]}),"\n",(0,t.jsx)(n.p,{children:"Instead of worrying about installation, configuration, or fine-tuning, developers simply declare which models to use when creating a store."}),"\n",(0,t.jsxs)(n.p,{children:["See the ",(0,t.jsx)(n.strong,{children:"Supported Models"})," section for the full list of available text and image models, including MiniLM, BGE variants, ResNet-50, and CLIP."]}),"\n",(0,t.jsxs)(n.p,{children:["This approach lets developers choose the right balance between ",(0,t.jsx)(n.strong,{children:"speed, accuracy, and modality support"})," depending on their workload."]}),"\n",(0,t.jsx)(n.h2,{id:"4-natural-querying",children:"4. Natural Querying"}),"\n",(0,t.jsx)(n.p,{children:"When querying, developers don\u2019t need to provide vectors\u2014they provide natural input. Ahnlich AI applies the configured query model, generates embeddings, and communicates with Ahnlich DB."}),"\n",(0,t.jsx)(n.h3,{id:"example--querying-with-text",children:"Example \u2013 Querying with Text:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:'GETSIMN "climate change effects on agriculture" IN news_store\n'})}),"\n",(0,t.jsxs)(n.p,{children:["Ahnlich AI generates embeddings for the query and performs a similarity search against all stored article embeddings.\nThis means queries like ",(0,t.jsx)(n.em,{children:"\u201cfind me similar jazz songs\u201d"})," or ",(0,t.jsx)(n.em,{children:"\u201cshow me products like this image\u201d"})," become possible without manual preprocessing."]})]})}function c(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}}}]);