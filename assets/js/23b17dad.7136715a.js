"use strict";(self.webpackChunkahnlich_web=self.webpackChunkahnlich_web||[]).push([[3655],{4373:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>a});const t=JSON.parse('{"id":"client-libraries/python/request-ai/create-store","title":"Create Store","description":"The CreateStore request is used to initialize a new AI-powered store.","source":"@site/docs/client-libraries/python/request-ai/create-store.md","sourceDirName":"client-libraries/python/request-ai","slug":"/client-libraries/python/request-ai/create-store","permalink":"/docs/client-libraries/python/request-ai/create-store","draft":false,"unlisted":false,"editUrl":"https://github.com/deven96/ahnlich/tree/main/web/ahnlich-web/docs/client-libraries/python/request-ai/create-store.md","tags":[],"version":"current","frontMatter":{"title":"Create Store"},"sidebar":"docsSidebar","previous":{"title":"List Stores","permalink":"/docs/client-libraries/python/request-ai/list-stores"},"next":{"title":"Set","permalink":"/docs/client-libraries/python/request-ai/set"}}');var i=r(74848),s=r(28453);const o={title:"Create Store"},l="Create Store",c={},a=[{value:"Key Notes",id:"key-notes",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components},{Details:r}=n;return r||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"create-store",children:"Create Store"})}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"CreateStore"})," request is used to ",(0,i.jsx)(n.strong,{children:"initialize a new AI-powered store"}),".\nUnlike the DB client (which deals with raw vector dimensions), the AI client lets you specify ",(0,i.jsx)(n.strong,{children:"pretrained AI models"})," to handle embedding generation and indexing."]}),"\n",(0,i.jsx)(n.p,{children:"This means you don\u2019t have to manage vectors manually \u2014 the AI service will automatically embed inputs using the selected models."}),"\n",(0,i.jsxs)(r,{children:[(0,i.jsx)("summary",{children:"Click to expand source code"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-py",children:'import asyncio\nfrom grpclib.client import Channel\nfrom ahnlich_client_py.grpc.services.ai_service import AiServiceStub\nfrom ahnlich_client_py.grpc.ai import query as ai_query\nfrom ahnlich_client_py.grpc.ai.models import AiModel\n\n\nasync def create_store():\n  async with Channel(host="127.0.0.1", port=1370) as channel:\n      client = AiServiceStub(channel)\n      response = await client.create_store(\n          ai_query.CreateStore(\n              store="test store",\n              query_model=AiModel.ALL_MINI_LM_L6_V2,\n              index_model=AiModel.ALL_MINI_LM_L6_V2,\n              predicates=["job"],\n              error_if_exists=True,\n              # Store original controls if we choose to store the raw inputs\n              # within the DB in order to be able to retrieve the originals again\n              # during query, else only store values are returned\n              store_original=True\n          )\n      )\n      print(response) # Unit()\n\n\nif __name__ == "__main__":\n  asyncio.run(create_store())\n'})})]}),"\n",(0,i.jsx)(n.h2,{id:"key-notes",children:"Key Notes"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"query_model"})," - model used for encoding query inputs during searches."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"index_model"})," - model used for encoding stored data vectors."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"predicates"}),' - metadata fields that can be filtered against (e.g., "',(0,i.jsx)(n.code,{children:"job"}),'").']}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"store_original"})," - if ",(0,i.jsx)(n.code,{children:"True"}),", the original raw input is stored alongside embeddings for later retrieval."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Response"})," - returns ",(0,i.jsx)(n.code,{children:"Unit()"})," on success."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This request is critical in AI workflows because it allows you to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Configure ",(0,i.jsx)(n.strong,{children:"semantic stores"})," with specialized embedding models."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Decide whether to preserve raw input text/images for retrieval."}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:["Build ",(0,i.jsx)(n.strong,{children:"intelligent"}),", ",(0,i.jsx)(n.strong,{children:"AI-driven search"})," and ",(0,i.jsx)(n.strong,{children:"recommendation systems"})," without managing embeddings manually."]}),"\n"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},28453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>l});var t=r(96540);const i={},s=t.createContext(i);function o(e){const n=t.useContext(s);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);